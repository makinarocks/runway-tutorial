{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f97820-61c1-4d8e-9194-67a7167844cb",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "aa4f4565-e5a9-4cd8-8262-f3fbc0de4829",
     "isComponent": false,
     "name": "",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision Pillow seaborn torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8f5c9-59c0-4a1f-a366-f36beff7cb60",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": true,
     "headerColor": "transparent",
     "id": "e2312a91-3ed9-41fc-ab53-156a10aa6fae",
     "isComponent": true,
     "name": "load dataset",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# RUNWAY_DATA_PATH was added to pipeline parameters.\n",
    "# Pipeline parameters can be used in the cell added as a pipeline component.\n",
    "# RUNWAY_DATA_PATH = \"/home/jovyan/workspace/dataset/sample-coco\"\n",
    "config_file = None\n",
    "for dirname, _, filenames in os.walk(RUNWAY_DATA_PATH):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith(\".json\"):\n",
    "            config_file = os.path.join(dirname, filename)\n",
    "\n",
    "if config_file is None:\n",
    "    raise ValueError(\"Can't find config file in given dataset\")\n",
    "\n",
    "coco = COCO(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329c1f08-c5b3-486a-a19d-f85b7a1977a9",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": true,
     "headerColor": "transparent",
     "id": "90b2c26c-4a14-42b5-85a3-f4692f79228a",
     "isComponent": true,
     "name": "extract image sample",
     "parents": [
      {
       "id": "e2312a91-3ed9-41fc-ab53-156a10aa6fae",
       "name": "load dataset"
      }
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "sample_image_path = next(Path(RUNWAY_DATA_PATH).glob(\"*.jpg\"))\n",
    "image_filename_list = [sample_image_path]\n",
    "\n",
    "img = Image.open(sample_image_path)\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec9dd7-1459-40a1-b52e-8589cc8162a3",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": true,
     "headerColor": "transparent",
     "id": "521ce9eb-a734-4411-9e80-29dbcc96ac24",
     "isComponent": true,
     "name": "define dataset",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "def get_transforms():\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, data_root, coco, transforms=None):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.transforms = transforms\n",
    "        # pre-loaded variables\n",
    "        self.coco = coco\n",
    "        self.ids = []\n",
    "        self._filter_ids()\n",
    "\n",
    "    def _filter_ids(self):\n",
    "        for img_id in list(sorted(self.coco.imgs.keys())):\n",
    "            if len(self.coco.getAnnIds(img_id)) > 0:\n",
    "                self.ids += [img_id]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # refer to https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        ann = self.coco.loadAnns(ann_ids)\n",
    "        img_path = self.data_root / self.coco.loadImgs(img_id)[0][\"file_name\"]\n",
    "        img = Image.open(img_path)\n",
    "        num_objs = len(ann)\n",
    "\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            boxes.append(\n",
    "                [\n",
    "                    ann[i][\"bbox\"][0],\n",
    "                    ann[i][\"bbox\"][1],\n",
    "                    ann[i][\"bbox\"][2] + ann[i][\"bbox\"][0],\n",
    "                    ann[i][\"bbox\"][3] + ann[i][\"bbox\"][1],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        areas = []\n",
    "        for i in range(num_objs):\n",
    "            areas.append(ann[i][\"area\"])\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.ones((num_objs,), dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "            \"area\": torch.as_tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.zeros((num_objs,), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        # transform image\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e82dd5-9d0f-4b8d-bc1d-2f32a895f8fe",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": true,
     "headerColor": "transparent",
     "id": "52605e22-3542-4dc1-95e5-0271494d60ae",
     "isComponent": true,
     "name": "data loader",
     "parents": [
      {
       "id": "521ce9eb-a734-4411-9e80-29dbcc96ac24",
       "name": "define dataset"
      },
      {
       "id": "90b2c26c-4a14-42b5-85a3-f4692f79228a",
       "name": "extract image sample"
      }
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define Train dataset\n",
    "data_root = Path(RUNWAY_DATA_PATH)\n",
    "dataset = COCODataset(data_root, coco, get_transforms())\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c47fb-5d60-4784-abf1-e729e7f7212d",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": true,
     "headerColor": "transparent",
     "id": "d6591d20-c44f-487b-9136-225d84949717",
     "isComponent": true,
     "name": "declare model",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "\n",
    "\n",
    "# Define local variables\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "try:\n",
    "    entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\n",
    "    model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\").to(device)\n",
    "except:\n",
    "    model = fasterrcnn_resnet50_fpn(weights=None, weights_backbone=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167481e5-d041-46b0-b5f9-55858c4d197b",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": true,
     "headerColor": "transparent",
     "id": "e6a4d0f5-5287-4d30-a54a-2181f074d480",
     "isComponent": true,
     "name": "train model",
     "parents": [
      {
       "id": "d6591d20-c44f-487b-9136-225d84949717",
       "name": "declare model"
      },
      {
       "id": "52605e22-3542-4dc1-95e5-0271494d60ae",
       "name": "data loader"
      }
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(params, lr=1e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for imgs, annotations in data_loader:\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "map_metric = MeanAveragePrecision().to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = []\n",
    "    annos = []\n",
    "    for imgs, annotations in data_loader:\n",
    "        pred = model(list(img.to(device) for img in imgs))\n",
    "        anno = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        preds.extend(pred)\n",
    "        annos.extend(anno)\n",
    "\n",
    "map_metric.update(preds, annos)\n",
    "map_score = map_metric.compute()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bee01b-92a0-4c20-a263-0eaa7b841385",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": true,
     "headerColor": "transparent",
     "id": "a3101996-2bee-4d03-b22b-edffd4ef29bc",
     "isComponent": true,
     "name": "model wrapper",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "class ModelWrapper:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def bytesarray_to_tensor(self, bytes_array: str):\n",
    "        # input : \"utf-8\" decoded bytes_array\n",
    "        encoded_bytes_array = bytes_array.encode(\"utf-8\")\n",
    "        # decode encoded_bytes_array with ascii code\n",
    "        img_64_decode = base64.b64decode(encoded_bytes_array)\n",
    "        # get image file and transform to tensor\n",
    "        image_from_bytes = Image.open(io.BytesIO(img_64_decode))\n",
    "        return transforms.ToTensor()(image_from_bytes).to(self.device)\n",
    "\n",
    "    def numpy_to_bytesarray(self, numpy_array):\n",
    "        numpy_array_bytes_array = numpy_array.tobytes()\n",
    "        numpy_array_64_encode = base64.b64encode(numpy_array_bytes_array)\n",
    "        bytes_array = numpy_array_64_encode.decode(\"utf-8\")\n",
    "        return bytes_array\n",
    "\n",
    "    def draw_detection(self, img_tensor, bboxes, labels, scores, out_img_file):\n",
    "        \"\"\"Draw detection result.\"\"\"\n",
    "        img_array = img_tensor.permute(1, 2, 0).numpy() * 255\n",
    "        img = Image.fromarray(img_array.astype(np.uint8))\n",
    "        \n",
    "        draw = ImageDraw.Draw(img)    \n",
    "        font = ImageFont.load_default()\n",
    "        bboxes = bboxes.cpu().numpy().astype(np.int32)\n",
    "        labels = labels.cpu().numpy()\n",
    "        scores = scores.cpu().numpy()\n",
    "        for box, label, score in zip(bboxes, labels, scores):        \n",
    "            draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\", width=1)  \n",
    "            text = f\"{label}: {score:.2f}\"\n",
    "            draw.text((box[0], box[1]), text, fill=\"red\", font=font)\n",
    "        img.save(out_img_file)\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, df):\n",
    "        self.model.eval()\n",
    "        # df is 1-d dataframe with bytes array\n",
    "        tensor_list = list((map(self.bytesarray_to_tensor, df.squeeze(axis=1).to_list())))\n",
    "\n",
    "        pred_images = []\n",
    "        pred_image_shape_c = []\n",
    "        pred_image_shape_h = []\n",
    "        pred_image_shape_w = []\n",
    "        pred_image_dtypes = []\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "\n",
    "        boxes_dtypes = []\n",
    "        labels_dtypes = []\n",
    "        scores_dtypes = []\n",
    "\n",
    "        for img in tensor_list:\n",
    "            output = self.model(img.unsqueeze(0))\n",
    "            detect_img = self.draw_detection(\n",
    "                img_tensor=img,\n",
    "                bboxes=output[0][\"boxes\"],\n",
    "                labels=output[0][\"labels\"],\n",
    "                scores=output[0][\"scores\"],\n",
    "                out_img_file=\"test.png\",\n",
    "            )\n",
    "            detect_img = np.array(detect_img)\n",
    "            h, w, c = detect_img.shape\n",
    "            box = output[0][\"boxes\"].cpu().numpy()\n",
    "            label = output[0][\"labels\"].cpu().numpy()\n",
    "            score = output[0][\"scores\"].cpu().numpy()\n",
    "\n",
    "            pred_images += [detect_img]\n",
    "            boxes += [box]\n",
    "            labels += [label]\n",
    "            scores += [score]\n",
    "\n",
    "            pred_image_shape_c += [c]\n",
    "            pred_image_shape_h += [h]\n",
    "            pred_image_shape_w += [w]\n",
    "\n",
    "            pred_image_dtypes += [str(detect_img.dtype)]\n",
    "            boxes_dtypes += [str(box.dtype)]\n",
    "            labels_dtypes += [str(label.dtype)]\n",
    "            scores_dtypes += [str(score.dtype)]\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        meta = pd.DataFrame({\n",
    "            \"pred_image_shape_c\": pred_image_shape_c,\n",
    "            \"pred_image_shape_h\": pred_image_shape_h,\n",
    "            \"pred_image_shape_w\": pred_image_shape_w,\n",
    "            \"output_dtype\": pred_image_dtypes,\n",
    "            \"boxes_dtypes\": boxes_dtypes,\n",
    "            \"labels_dtypes\": labels_dtypes,\n",
    "            \"scores_dtypes\": scores_dtypes,\n",
    "        })\n",
    "        img_byte = pd.DataFrame({\n",
    "            \"output\": pred_images,\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"scores\": scores,\n",
    "            # \"true\": tensor_list,\n",
    "        }).applymap(lambda x: self.numpy_to_bytesarray(x))\n",
    "        return pd.concat([meta, img_byte], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeae8a7-550f-4df7-a540-09e0e528a6fd",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": true,
     "headerColor": "transparent",
     "id": "10f83f49-5438-4879-981b-f10dab1d9348",
     "isComponent": true,
     "name": "image to bytearray",
     "parents": []
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def convert_image_to_bytearray(img_binary):\n",
    "    image_64_encode = base64.b64encode(img_binary)\n",
    "    bytes_array = image_64_encode.decode(\"utf-8\")\n",
    "    return bytes_array\n",
    "\n",
    "\n",
    "def images_to_bytearray_df(image_filename_list: list):\n",
    "    df_list = []\n",
    "    for img_filename in image_filename_list:\n",
    "        image = open(img_filename, \"rb\")  # open binary file in read mode\n",
    "        image_read = image.read()\n",
    "        df_list.append(convert_image_to_bytearray(image_read))\n",
    "    return pd.DataFrame(df_list, columns=[\"image_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece8dd7-ba8a-48d2-9b32-d62b629301c7",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": true,
     "headerColor": "transparent",
     "id": "8e27b735-4ed5-4687-943b-ddf629431580",
     "isComponent": true,
     "name": "show sample inference",
     "parents": [
      {
       "id": "10f83f49-5438-4879-981b-f10dab1d9348",
       "name": "image to bytearray"
      },
      {
       "id": "a3101996-2bee-4d03-b22b-edffd4ef29bc",
       "name": "model wrapper"
      },
      {
       "id": "e6a4d0f5-5287-4d30-a54a-2181f074d480",
       "name": "train model"
      }
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "device = \"cpu\"\n",
    "serve_model = ModelWrapper(model=model, device=device)\n",
    "\n",
    "# make input sample\n",
    "input_sample = images_to_bytearray_df(image_filename_list)\n",
    "\n",
    "# For inference\n",
    "pred = serve_model.predict(input_sample)\n",
    "\n",
    "output = pred.loc[0]\n",
    "data, dtype = output[\"output\"], output[\"output_dtype\"]\n",
    "c, h, w = output[\"pred_image_shape_c\"], output[\"pred_image_shape_h\"], output[\"pred_image_shape_w\"]\n",
    "\n",
    "type_dict = {\"uint8\": np.uint8, \"float32\": np.float32, \"int64\": np.int64}\n",
    "pred_decode = base64.b64decode(data)\n",
    "pred_array = np.frombuffer(pred_decode, dtype=type_dict[dtype])\n",
    "\n",
    "img = Image.fromarray(pred_array.reshape(h, w, c))\n",
    "\n",
    "imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685ebf7-24a9-4e88-8198-3862e656a5c2",
   "metadata": {
    "canvas": {
     "comments": [],
     "componentType": "CodeCell",
     "copiedOriginId": null,
     "diskcache": false,
     "headerColor": "transparent",
     "id": "a0094bf1-1d6d-450c-b6a7-edce002b3e35",
     "isComponent": true,
     "name": "log model",
     "parents": [
      {
       "id": "8e27b735-4ed5-4687-943b-ddf629431580",
       "name": "show sample inference"
      }
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import runway\n",
    "\n",
    "del map_score[\"classes\"]\n",
    "runway.start_run()\n",
    "runway.log_metrics(map_score)\n",
    "\n",
    "runway.log_model(model_name=\"my-detection-model\", model=serve_model, input_samples={\"predict\": input_sample})\n",
    "\n",
    "runway.stop_run()"
   ]
  }
 ],
 "metadata": {
  "canvas": {
   "colorPalette": [
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit",
    "inherit"
   ],
   "parameters": [
    {
     "metadata": {
      "dataset_version_id": 496,
      "is_latest": true,
      "resource_id": 496,
      "resource_name": "minio-dataset",
      "resource_type": "runway_dataset"
     },
     "name": "RUNWAY_DATA_PATH",
     "type": "str",
     "value": "\"/home/jovyan/workspace/dataset/minio-dataset/latest\""
    },
    {
     "metadata": null,
     "name": "N_EPOCHS",
     "type": "int",
     "value": "1"
    }
   ],
   "version": "1.0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
